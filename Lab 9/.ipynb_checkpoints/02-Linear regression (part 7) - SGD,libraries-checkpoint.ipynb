{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (laboratory instruction)\n",
    "## Linear regression (part 7)\n",
    "\n",
    "### Submission\n",
    "\n",
    "<u>Submission:</u>\n",
    "\n",
    "Compress all files into **single zip** archive and submit via Wikamp. See below the content of the archive (replace the `name` and `surname` with proper values):\n",
    "```\n",
    "ðŸ“‚ name.surname.zip\n",
    "+-- ðŸ“œ 02-Linear regression (part 7).ipynb\n",
    "```\n",
    "\n",
    "<u>Grades</u>\n",
    "\n",
    "| Percentage of all points | mark |\n",
    "| :----                    | ---: |\n",
    "| [0-50)   | 2   |\n",
    "| [50-60)  | 3   |\n",
    "| [60-70)  | 3.5 |\n",
    "| [70-80)  | 4   |\n",
    "| [80-90)  | 4.5 |\n",
    "| [90-100] | 5   |\n",
    "\n",
    "<u>Penalties</u>\n",
    "\n",
    "* `mark - 0.5` if tasks are submitted after laboratory (but less than 7 days); \n",
    "* `mark - 1` if tasks are submitted after one week (>=7 days but < 14 days);\n",
    "* `mark - 1.5` if tasks are submitted later than two weeks (>=14 days).\n",
    "\n",
    "<u>Warning:</u>\n",
    "\n",
    "It is NOT allowed to share your .ipynb file with other students nor Internet. All students should download the exercise files directly from WIKAMP. Group work is considered as plagiarism.\n",
    "\n",
    "<u>Plagiarism Disclaimer:</u>\n",
    "\n",
    "I hereby declare that this exercise is my own and autonomous work. I am aware of the consequences and I am familiar with the university regulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal\n",
    "There are two goals of this laboratory:\n",
    "\n",
    "1. Implement Stochastic Gradient Descent (Mini-batch Gradient Descent).\n",
    "2. Use ML libraries to perform Linear Regression.\n",
    "\n",
    "\n",
    "### Task 0. Read the dataset from file (*0 points*).\n",
    "Read the boston houses dataset using scikit-learn library.\n",
    "\n",
    "**Tip**: You can use below code to read the data:\n",
    "\n",
    "```python\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "\n",
    "dataset = sklearn.datasets.load_boston()\n",
    "\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(dataset['data'], dataset['target'], test_size=0.5, random_state=123, shuffle=True)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253, 13) (253, 13)\n"
     ]
    }
   ],
   "source": [
    "# >>> WRITE YOUR CODE BELOW <<<\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "\n",
    "dataset = sklearn.datasets.load_boston()\n",
    "\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(dataset['data'], dataset['target'], test_size=0.5, random_state=123, shuffle=True)\n",
    "\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Implement Stochastic Gradient Descent (Mini-batch Gradient Descent).\n",
    "\n",
    "**Tasks**: \n",
    "* 2.1. Implement the Mini-batch GD\n",
    "* 2.2. Prepare the data (if needed).\n",
    "* 2.3. Visualize: \n",
    "    * a) the data, \n",
    "    * b) results, \n",
    "    * c) cost functions.\n",
    "* 2.4. Check how the algorithm behaves for different batch sizes.\n",
    "\n",
    "Till now your Gradient Descent algorithm more or less looks like below:\n",
    "\n",
    "```python\n",
    "for i in range(epochs):\n",
    "    yp = fun(x)\n",
    "    c  = cost(yp, yt)\n",
    "    w -= eta * grad(x, yt, yp)\n",
    "    # ...\n",
    "```\n",
    "\n",
    "Gradient Descent performs the update for all samples at once, one time in the epoch. It may not be possible if the training dataset is large (i.e. the dataset is larger than the \\[V\\] RAM). The Stochastic Gradient Descent (SGD) helps with that by processing the data one-by-one and updating the weights at each iteration:\n",
    "\n",
    "```python\n",
    "for i in range(epochs):\n",
    "    for j in range(len(x)):\n",
    "        bx = x[j]\n",
    "        by = y[j]\n",
    "        yp = fun(bx)\n",
    "        c  = cost(yp, by)\n",
    "        w -= eta * grad(x, by, yp)\n",
    "    # ...\n",
    "```\n",
    "\n",
    "Another variant is the Mini-batch Gradient Descent that uses a couple (a batch) samples at each iteration (the number of iterations equals $\\lceil m / b \\rceil$, where $m$ is the number of samples and $b$ is the batch size):\n",
    "\n",
    "```python\n",
    "for i in range(epochs):\n",
    "    for j in range(int(np.ceil(len(x_train)/batch_size))):\n",
    "        bx = x[j*batch_size:(j+1)*batch_size]\n",
    "        by = y[j*batch_size:(j+1)*batch_size]\n",
    "        yp = fun(bx)\n",
    "        c  = cost(yp, by)\n",
    "        w -= eta * grad(x, by, yp)\n",
    "    # ...\n",
    "```\n",
    "\n",
    "Remarks: The Mini-batch Gradient Descent is the same as SGD if $b=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-581adb1e65b5>:26: RuntimeWarning: invalid value encountered in subtract\n",
      "  w -= lr * grad(batch_x, predictions, batch_y)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# >>> WRITE YOUR CODE BELOW <<<\n",
    "from numba import njit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@njit\n",
    "def mse(predictions, targets):\n",
    "    return np.square(np.subtract(predictions, targets)).mean()\n",
    "@njit\n",
    "def grad(x, predictions, targets):\n",
    "    return (2 / targets.shape[0]) * (x.T @ np.subtract(predictions, targets))\n",
    "\n",
    "#w = np.random.uniform(size=(3,))\n",
    "w = np.zeros(shape=(13,))\n",
    "lr = 1e-2\n",
    "epochs = 100000\n",
    "history = {'train': [], 'test': []}\n",
    "batch_size = 32\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "for i in range(epochs):\n",
    "    for j in range(int(np.ceil(len(x_train)/batch_size))):\n",
    "        batch_x = x_train[j*batch_size:(j+1)*batch_size]\n",
    "        batch_y = y_train[j*batch_size:(j+1)*batch_size]\n",
    "        predictions = batch_x @ w.T\n",
    "        train_error = mse(predictions, batch_y)\n",
    "        w -= lr * grad(batch_x, predictions, batch_y)\n",
    "        history['train'].append(train_error)\n",
    "        \n",
    "        \n",
    "        #test_predictions = x_test @ w.T\n",
    "        #test_error = mse(test_predictions, y_test)\n",
    "        #history['test'].append(test_error)\n",
    "\n",
    "plt.plot(history['train'], label='Train Loss')\n",
    "#plt.plot(history['test'], label='Test Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Train linear regression model.\n",
    "\n",
    "2. Use at least two widely used ML libraries to perform Linear Regression (you may also use those that are not mentioned in the list below) and describe the differences between them:\n",
    "    * Scikit-learn, a couple of options available:\n",
    "        * LinearRegression \n",
    "        * SGDRegressor\n",
    "        * Ridge\n",
    "        * Lasso\n",
    "        * ElasticNet\n",
    "    * Scipy\n",
    "        * scipy.linalg.lstsq\n",
    "        * scipy.stats.linregress\n",
    "    * Tensorflow\n",
    "    * Pytorch\n",
    "    * JAX\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> WRITE YOUR CODE BELOW <<<\n"
   ]
  }
 ],
 "metadata": {
  "author": {
   "emails": [
    "robert.susik@p.lodz.pl",
    "rsusik@kis.p.lodz.pl"
   ],
   "name": "Robert Susik"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
