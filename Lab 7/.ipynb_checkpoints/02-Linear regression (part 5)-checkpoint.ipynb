{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (laboratory instruction)\n",
    "## Linear regression (part 5)\n",
    "\n",
    "### Submission\n",
    "\n",
    "<u>Submission:</u>\n",
    "\n",
    "Compress all files into **single zip** archive and submit via Wikamp. See below the content of the archive (replace the `name` and `surname` with proper values):\n",
    "```\n",
    "ðŸ“‚ name.surname.zip\n",
    "+-- ðŸ“œ 02-Linear regression (part 5).ipynb\n",
    "+-- ðŸ–¼ houses.npz\n",
    "```\n",
    "\n",
    "<u>Grades</u>\n",
    "\n",
    "| Percentage of all points | mark |\n",
    "| :----                    | ---: |\n",
    "| [0-50)   | 2   |\n",
    "| [50-60)  | 3   |\n",
    "| [60-70)  | 3.5 |\n",
    "| [70-80)  | 4   |\n",
    "| [80-90)  | 4.5 |\n",
    "| [90-100] | 5   |\n",
    "\n",
    "<u>Penalties</u>\n",
    "\n",
    "* `mark - 0.5` if tasks are submitted after laboratory (but less than 7 days); \n",
    "* `mark - 1` if tasks are submitted after one week (>=7 days but < 14 days);\n",
    "* `mark - 1.5` if tasks are submitted later than two weeks (>=14 days).\n",
    "\n",
    "<u>Warning:</u>\n",
    "\n",
    "It is NOT allowed to share your .ipynb file with other students nor Internet. All students should download the exercise files directly from WIKAMP. Group work is considered as plagiarism.\n",
    "\n",
    "<u>Plagiarism Disclaimer:</u>\n",
    "\n",
    "I hereby declare that this exercise is my own and autonomous work. I am aware of the consequences and I am familiar with the university regulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal\n",
    "Implementation of multivariate linear regression for house price prediction.\n",
    "\n",
    "### Dataset\n",
    "Read the data from `houses.npz` file. This is a multivariate data with information about the house (flats) prices. The columns in $x$ contain following information, respectively:\n",
    "* the area,\n",
    "* distance to city centre,\n",
    "* standard,\n",
    "* distance to highway.\n",
    "\n",
    "The $y$ contains the price of the flat.\n",
    "\n",
    "**Tip**: You can use below code to read the data:\n",
    "\n",
    "```python\n",
    "with open('houses.npz', 'rb') as f:\n",
    "    data = np.load(f)\n",
    "    x, y = data['x'], data['y']\n",
    "```\n",
    "\n",
    "### Task. Implement the multivariate linear regression.\n",
    "Implement linear regression function $f$ that takes variable number of parameters. This is simply a weighted sum of input features, plus a constant (the bias).\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "f_\\theta(x) = \\theta_0x_0 + \\theta_1x_1 + \\cdots + \\theta_{n-1}x_{n-1} + \\theta_{n}x_n\n",
    "\\label{eq:fun} \\tag{1}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    ", where $\\theta$ are the model parameters (parameter vector), $x$ are the features (feature vector), $n$ is the number of features. Note that $\\theta_0x_0$ (depending on the implementation) represents the bias where the $x_0$ always equals 1 and $\\theta_0$ is the actual bias.\n",
    "\n",
    "To add a column with only ones you can use `hstack` function: `np.hstack((np.ones(len(x)).reshape(-1, 1), x))`.\n",
    "\n",
    "In this case you will have to calculate the partial derivatives for all the parameters. The vector which contains all the partial derivatives is called gradient:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\nabla = \\bigg{\\lgroup}\\frac{\\partial}{\\partial \\theta_0 MSE(\\theta)}, \\frac{\\partial}{\\partial \\theta_1 MSE(\\theta)}, \\cdots, \\frac{\\partial}{\\partial \\theta_n MSE(\\theta)}\\bigg{\\rgroup}\n",
    "\\label{eq:gradient} \\tag{2}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "The next step can then be calculated by following formula:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\theta' = \\theta - \\alpha\\nabla_\\theta MSE(\\theta)\n",
    "\\label{eq:nextstep} \\tag{3}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "It can be even written as simplified formula:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)}\n",
    "\\label{eq:nextstepsimple} \\tag{4}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    ", where $j$ is the number of features, $\\alpha$ is the learning rate, and $m$ is the number of samples.\n",
    "\n",
    "\n",
    "#### Vectorized form.\n",
    "\n",
    "It is likely that the implementation of above will work very slowly if implemented iteratively. Implement the solution using vectorized form. In this case the equation $\\eqref{eq:fun}$ can be done by vector multiplication:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "f_\\theta(x) =\\begin{bmatrix}\\theta_0 \\hspace{2em} \\theta_1 \\hspace{2em} ... \\hspace{2em} \\theta_n\\end{bmatrix}\\begin{bmatrix}x_0 \\newline x_1 \\newline \\vdots \\newline x_n\\end{bmatrix}= \\theta^T x\n",
    "\\label{eq:funvec} \\tag{5}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Thus, the cost function is:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "MSE = \\frac{1}{m}\\sum(\\theta^Tx^{(i)} - y^{(i)})^2\n",
    "\\label{eq:costvec} \\tag{6}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "And the partial derivatives can be calculated as follows:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial}{\\partial\\theta_j}MSE = \\frac{2}{m}\\sum(\\theta^Tx^{(i)} - y^{(i)})x_j^{(i)}\n",
    "\\label{eq:dervec} \\tag{7}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Now, we can define the gradient:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\nabla = \\frac{2}{m}x^T(X\\theta - y)\n",
    "\\label{eq:gradientvec} \\tag{8}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Remark: There is possibility that your calculation may differ if you use different shapes of the vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# >>> WRITE YOUR CODE BELOW <<<\n"
   ]
  }
 ],
 "metadata": {
  "author": {
   "emails": [
    "robert.susik@p.lodz.pl",
    "rsusik@kis.p.lodz.pl"
   ],
   "name": "Robert Susik"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
